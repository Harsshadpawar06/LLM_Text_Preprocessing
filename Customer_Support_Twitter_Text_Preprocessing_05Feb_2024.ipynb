{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS PART of DATA preprocessing we will be focusing on below data clearning techniques:\n",
    "\n",
    "1.  tokenization\n",
    "2.  lowercase / uppercase\n",
    "3.  emojis\n",
    "4.  pancuations\n",
    "5.  html,url\n",
    "6.  stopwords\n",
    "7.  abbravation or slang\n",
    "8.  steemming and lemmetization\n",
    "9.  spelling correction\n",
    "10. whitspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data from the Kaggle dataset\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"testdata/twcs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1004110</th>\n",
       "      <td>1112648</td>\n",
       "      <td>216217</td>\n",
       "      <td>True</td>\n",
       "      <td>Sat Oct 14 05:55:58 +0000 2017</td>\n",
       "      <td>@TMobileHelp I DMed you</td>\n",
       "      <td>1112646</td>\n",
       "      <td>1112649.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2423461</th>\n",
       "      <td>2592262</td>\n",
       "      <td>AppleSupport</td>\n",
       "      <td>False</td>\n",
       "      <td>Fri Nov 17 01:45:00 +0000 2017</td>\n",
       "      <td>@734151 We'd love to look into any issues you'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2592263.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852697</th>\n",
       "      <td>2009347</td>\n",
       "      <td>AmericanAir</td>\n",
       "      <td>False</td>\n",
       "      <td>Wed Oct 18 20:15:10 +0000 2017</td>\n",
       "      <td>@594158 You bet! Tweet us if you need anything...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837688</th>\n",
       "      <td>1993229</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>False</td>\n",
       "      <td>Fri Nov 03 13:08:12 +0000 2017</td>\n",
       "      <td>@589887 Along with the SC (supplier code) numb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827392</th>\n",
       "      <td>921429</td>\n",
       "      <td>338742</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 10 12:37:58 +0000 2017</td>\n",
       "      <td>Oh my god this @115873 smells like buckets of ...</td>\n",
       "      <td>921428</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id     author_id  inbound                      created_at  \\\n",
       "1004110   1112648        216217     True  Sat Oct 14 05:55:58 +0000 2017   \n",
       "2423461   2592262  AppleSupport    False  Fri Nov 17 01:45:00 +0000 2017   \n",
       "1852697   2009347   AmericanAir    False  Wed Oct 18 20:15:10 +0000 2017   \n",
       "1837688   1993229         Tesco    False  Fri Nov 03 13:08:12 +0000 2017   \n",
       "827392     921429        338742     True  Tue Oct 10 12:37:58 +0000 2017   \n",
       "\n",
       "                                                      text response_tweet_id  \\\n",
       "1004110                            @TMobileHelp I DMed you           1112646   \n",
       "2423461  @734151 We'd love to look into any issues you'...               NaN   \n",
       "1852697  @594158 You bet! Tweet us if you need anything...               NaN   \n",
       "1837688  @589887 Along with the SC (supplier code) numb...               NaN   \n",
       "827392   Oh my god this @115873 smells like buckets of ...            921428   \n",
       "\n",
       "         in_response_to_tweet_id  \n",
       "1004110                1112649.0  \n",
       "2423461                2592263.0  \n",
       "1852697                2009348.0  \n",
       "1837688                1993230.0  \n",
       "827392                       NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will start with the lowercase conversion:\n",
    "# 2.  lowercase / uppercase\n",
    "\n",
    "data[\"text\"]=data[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2570589    @767993 sorry for the late reply and the poor ...\n",
       "1019550                 @americanair not allowing carryons üò§\n",
       "2363043                   @235907 fantastic costumes, craig!\n",
       "678007     @115900 and why should i have to pay for a tec...\n",
       "1806389    @115714 your service fucking sucks. like how i...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will now remove html tags and urls:\n",
    "# 5.  html,url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text1):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub(\"\",text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1270525    @270963 @441906 @58398 glad to hear it was a g...\n",
       "1299982    @microsofthelps no. i am trying to connect oth...\n",
       "1634802    @537318 hi tim. i'm not aware of any issues wi...\n",
       "2749542                   @coxhelp it says iÔ∏è can‚Äôt dm y‚Äôall\n",
       "355006     ever since @3126 changed their name to @1625 t...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586470     @276979 sorry to hear that, paul - keep in min...\n",
       "283455     @193244 we offer support via twitter in englis...\n",
       "1442331    @454572 thanks for reaching out! we've got you...\n",
       "1260887    @443524 we're awaiting for your response to re...\n",
       "2548690                                 @british_airways pls\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will remove the punctuation:\n",
    "#4.  punctuation\n",
    "\n",
    "import string\n",
    "exclude=string.punctuation\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,\"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190080     169497 prime twoday shipping refers to the tra...\n",
       "1927235    askseagate i have sent you like a dozen mails ...\n",
       "2022465    124134 231644 hi keiran  i am really sorry for...\n",
       "2259098                                tmobilehelp doing now\n",
       "1927963    569515 well thats not nice whats going on plea...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will be removing Emojis:\n",
    "# 3.  Emojis\n",
    "\n",
    "import emoji\n",
    "def remove_emoji(text):\n",
    "    clean_text=emoji.demojize(text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527668     115817 are you fake delivery service three day...\n",
       "276162     askebay case no 5148548995 and someone was mea...\n",
       "2151380                                 gwrhelp any thoughts\n",
       "540526     amazonhelp ya lo hice y la soluci√≥n recibida h...\n",
       "913050     359923 thats odd are you having any issues wit...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AS we done data cleaning for punchuation earlier we will rerun it to remove the : and _ from the text\n",
    "data[\"text\"]=data[\"text\"].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1595653    527096 here‚Äôs what you can do to work around t...\n",
       "2468150    askplaystation this not helping really please ...\n",
       "2599786    774662 greggsofficial ‚Äúwe‚Äù  u kidding me ur no...\n",
       "808746       mcdonalds 334003 dont forget the szechuan sauce\n",
       "2478628    738539 hello 738539 apologies for the experien...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2646847    564173 i do apologize for the inconvenience an...\n",
       "720262     311867 hi gareth i will certainly look into th...\n",
       "1048878    393113 thanks could you also dm us your accoun...\n",
       "1199452           127063 428174 londonmidland i bet they did\n",
       "751301     verizonsupport i‚Äôm not an idiot cuz i‚Äôm in cor...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will update abbravation or slang words:\n",
    "# 7.  abbravation\n",
    "\n",
    "chat_words={\n",
    "   \" AFAIK\":\"As Far As I Know\",\n",
    "\"AFK\": \"Away From Keyboard\",\n",
    "\"ASAP\":\"As Soon As Possible\",\n",
    "\"BTW\":\"By The Way\",\n",
    "\"B4\":\"Before\",\n",
    "\"LAMO\":\"Laugh My A.. Off\",\n",
    "\"FYI\":\"For your information\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    \n",
    "    new_text=[]\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"text\"] = data[\"text\"].apply(lambda x: ' '.join(x))\n",
    "data[\"text\"] = data[\"text\"].apply(chat_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394689     221691 really sorry for the delay as we are ex...\n",
       "1438011      486940 how long have you noticed this issue ddg\n",
       "1101291    264277 hi tiffany you can call using the follo...\n",
       "1257512    tesco really disappointed amp annoyed to find ...\n",
       "278264     164814 amazonhelp they are not amazonhelp they...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will rectify spelling mistakes:\n",
    "# 6.  spelling mistakes\n",
    "\n",
    "# The TextBlob library's spell correction function can be computationally expensive, especially for large datasets. Instead of applying the correction directly to each text entry in your dataframe using the lambda function, you can speed up the process by using parallelization.\n",
    "\n",
    "# Use the swifter library for parallel processing\n",
    "# data[\"text\"] = data[\"text\"].swifter.apply(spell_correction)\n",
    "\n",
    "# as it is taking more time to demonstrate so we will verify if function is correct by applying on only 1st 10 reocrds\n",
    "\n",
    "# Select the first 10 records for processing\n",
    "\n",
    "import textblob\n",
    "import swifter\n",
    "from textblob import TextBlob\n",
    "\n",
    "def spell_correction(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "# Select the first 1000 records for processing\n",
    "subset_data = data.head(1000).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [05:09<00:00,  3.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use the swifter library for parallel processing\n",
    "subset_data.loc[:, \"text\"] = subset_data[\"text\"].swifter.apply(spell_correction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669    virgintrains thanks we are an hour away so fin...\n",
       "525    115886 your receiving the same answers because...\n",
       "630    115914 may for mobile tuesdays less get you ta...\n",
       "254    airasiasupport lost my booking chum green lee ...\n",
       "790         115964 creeeeepy thanks for stopping by beck\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# removing stop words\n",
    "# 6.  stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text, language='english'):\n",
    "    # Load stopwords for the specified language\n",
    "    stop_words = set(nltk.corpus.stopwords.words(language))\n",
    "\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Remove extra spaces\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2811774"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2219591                               atviassist little push\n",
       "492321     250003 115714 115725 cheaper thought verizon w...\n",
       "2232868            689823 apologies delay tams reached dm sk\n",
       "1613714    tmobilehelp thanks 4 always awesome love tmobi...\n",
       "608110         282736 got ya able download check see list bl\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the updated 'data' DataFrame\n",
    "data.sample(5)[\"text\"]\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# now we will tokaniize the words:\n",
    "# 1.  Tokenization\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the function for tokenization\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2317826                                          [askpaypal]\n",
       "1324642    [requested, refund, morrisons, delivery, 15, i...\n",
       "1571739               [434963, hey, weve, followed, via, dm]\n",
       "2102428    [got, ta, applaud, southwestair, online, disco...\n",
       "2439792    [737950, would, love, look, please, send, us, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At last we will doing steemming and lemmetization\n",
    "# 8.  steemming and lemmetization\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Function for stemming using NLTK's PorterStemmer\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Function for lemmatization using TextBlob\n",
    "def lemmatize_text(text):\n",
    "    # Join the list of words into a string before creating a TextBlob\n",
    "    blob = TextBlob(' '.join(text))\n",
    "    lemmatized_words = [word.lemmatize() for word in blob.words]\n",
    "    return ' '.join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to the \"text\" column and update it\n",
    "data[\"text\"] = data[\"text\"].apply(stem_text)\n",
    "# as we have less time we will go with steming only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming(word_list):\n",
    "    obj = PorterStemmer()\n",
    "    stem_words = [obj.stem(word) for word in word_list]\n",
    "    return ' '.join(stem_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[\"text\"] = data[\"text\"].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2803987    115786 15913 115890 153744 xboxsupport would o...\n",
       "1292165    450914 sorri disrupt journey pleas follow link...\n",
       "722490     312452 hi would happi help look pleas send dm ...\n",
       "598432                           amazonhelp thank smilingfac\n",
       "1609839             136047 great let us know issu happi help\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1208780 argoshelp thank chanc detail informatio... 731214 314680 thank reach unfortun can‚Äôt r... 2041280 643414 ohh look delish enjoy flight thank cho... 2636800 comcastoutag comcastcar explain outag 927359 363538 wed love help issu come across devic\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming(text):\n",
    "    obj = PorterStemmer()\n",
    "    words_list = text.split()  # Split the text into a list of words\n",
    "    stem_words = [obj.stem(word) for word in words_list]\n",
    "    return ' '.join(stem_words)\n",
    "\n",
    "# Example usage\n",
    "text_example = \"1208780 argoshelpers thanks chance detailed informatio... 731214 314680 thanks reaching unfortunately can‚Äôt r... 2041280 643414 ohh looks delish enjoy flight thank cho... 2636800 comcastoutage comcastcares explain outage 927359 363538 wed love help issue coming across devic\"\n",
    "\n",
    "# Applying the function to the example text\n",
    "result = stemming(text_example)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1208780    argoshelpers thanks chance detailed informatio...\n",
       "731214     314680 thanks reaching unfortunately can ‚Äô t r...\n",
       "2041280    643414 ohh looks delish enjoy flight thank cho...\n",
       "2636800            comcastoutage comcastcares explain outage\n",
       "927359     363538 wed love help issue coming across devic...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to the \"text\" column and update it\n",
    "data[\"text\"] = data[\"text\"].apply(lemmatize_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
